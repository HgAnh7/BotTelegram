import requests
import json
import os
import time
import asyncio
import aiohttp
from datetime import datetime
from telegram import Update
from telegram.ext import Application, CommandHandler, ContextTypes
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Set, Optional, Tuple

# C·∫•u h√¨nh logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Thay th·∫ø b·∫±ng token bot c·ªßa b·∫°n
BOT_TOKEN = os.getenv("TELEGRAM_TOKEN")

class URLCollector:
    def __init__(self):
        self.collected_urls: Set[str] = set()
        self.is_collecting = False
        self.session = None
        
    async def create_session(self):
        """T·∫°o aiohttp session ƒë·ªÉ t√°i s·ª≠ d·ª•ng connection"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=5, connect=2)
            connector = aiohttp.TCPConnector(
                limit=100,  # T·ªïng s·ªë connection
                limit_per_host=20,  # Connection per host
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
            self.session = aiohttp.ClientSession(
                timeout=timeout,
                connector=connector,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
    
    async def close_session(self):
        """ƒê√≥ng session"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def fetch_url_from_api_async(self, api_url: str, url_key: str = "url") -> Optional[str]:
        """
        G·ªçi API async v√† l·∫•y URL t·ª´ JSON response
        """
        try:
            await self.create_session()
            
            async with self.session.get(api_url) as response:
                if response.status == 200:
                    try:
                        data = await response.json()
                        url = self.extract_url_from_json(data, url_key)
                        return url
                    except (json.JSONDecodeError, aiohttp.ContentTypeError):
                        logger.error("Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá")
                        return None
                else:
                    logger.warning(f"API tr·∫£ v·ªÅ status code: {response.status}")
                    return None
                    
        except asyncio.TimeoutError:
            logger.error("Request timeout")
            return None
        except Exception as e:
            logger.error(f"L·ªói khi g·ªçi API: {e}")
            return None
    
    def fetch_url_from_api_sync(self, api_url: str, url_key: str = "url", timeout: int = 5) -> Optional[str]:
        """
        Phi√™n b·∫£n sync backup cho tr∆∞·ªùng h·ª£p c·∫ßn thi·∫øt
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(api_url, headers=headers, timeout=timeout)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    url = self.extract_url_from_json(data, url_key)
                    return url
                except json.JSONDecodeError:
                    logger.error("Response kh√¥ng ph·∫£i JSON h·ª£p l·ªá")
                    return None
            else:
                logger.warning(f"API tr·∫£ v·ªÅ status code: {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("Request timeout")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"L·ªói khi g·ªçi API: {e}")
            return None
    
    def extract_url_from_json(self, data, url_key: str) -> Optional[str]:
        """
        Tr√≠ch xu·∫•t URL t·ª´ JSON data (h·ªó tr·ª£ nested) - T·ªëi ∆∞u h√≥a
        """
        if isinstance(data, dict):
            # Ki·ªÉm tra key tr·ª±c ti·∫øp tr∆∞·ªõc
            if url_key in data:
                return data[url_key]
            
            # T√¨m trong nested objects
            for value in data.values():
                if isinstance(value, (dict, list)):
                    result = self.extract_url_from_json(value, url_key)
                    if result:
                        return result
        elif isinstance(data, list):
            for item in data:
                result = self.extract_url_from_json(item, url_key)
                if result:
                    return result
        
        return None
    
    async def collect_urls_async_batch(self, api_url: str, num_requests: int, url_key: str = "url", 
                                     batch_size: int = 20, progress_callback=None) -> Tuple[int, int, int]:
        """
        Thu th·∫≠p URLs s·ª≠ d·ª•ng async batch processing
        """
        self.is_collecting = True
        new_urls_count = 0
        duplicate_count = 0
        error_count = 0
        
        await self.create_session()
        
        try:
            # Chia th√†nh c√°c batch ƒë·ªÉ x·ª≠ l√Ω
            for batch_start in range(0, num_requests, batch_size):
                if not self.is_collecting:
                    break
                
                batch_end = min(batch_start + batch_size, num_requests)
                current_batch_size = batch_end - batch_start
                
                # T·∫°o tasks cho batch hi·ªán t·∫°i
                tasks = [
                    self.fetch_url_from_api_async(api_url, url_key) 
                    for _ in range(current_batch_size)
                ]
                
                # Ch·∫°y batch v·ªõi timeout
                try:
                    results = await asyncio.wait_for(
                        asyncio.gather(*tasks, return_exceptions=True),
                        timeout=30.0  # Timeout cho c·∫£ batch
                    )
                except asyncio.TimeoutError:
                    logger.error("Batch timeout")
                    error_count += current_batch_size
                    continue
                
                # X·ª≠ l√Ω k·∫øt qu·∫£
                for result in results:
                    if isinstance(result, Exception):
                        error_count += 1
                    elif result:
                        if result not in self.collected_urls:
                            self.collected_urls.add(result)
                            new_urls_count += 1
                            logger.info(f"Th√™m URL m·ªõi: {result}")
                        else:
                            duplicate_count += 1
                    else:
                        error_count += 1
                
                # Callback ti·∫øn tr√¨nh
                if progress_callback:
                    progress_callback(batch_end, num_requests, new_urls_count, duplicate_count, error_count)
                
                # Delay nh·ªè gi·ªØa c√°c batch
                if batch_end < num_requests:
                    await asyncio.sleep(0.1)
        
        finally:
            self.is_collecting = False
            await self.close_session()
        
        return new_urls_count, duplicate_count, error_count
    
    def collect_urls_threaded(self, api_url: str, num_requests: int, url_key: str = "url", 
                            max_workers: int = 10, progress_callback=None) -> Tuple[int, int, int]:
        """
        Thu th·∫≠p URLs s·ª≠ d·ª•ng ThreadPoolExecutor (fallback cho sync)
        """
        self.is_collecting = True
        new_urls_count = 0
        duplicate_count = 0
        error_count = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # T·∫°o futures
            futures = [
                executor.submit(self.fetch_url_from_api_sync, api_url, url_key, 5)
                for _ in range(num_requests)
            ]
            
            # X·ª≠ l√Ω k·∫øt qu·∫£ khi ho√†n th√†nh
            for i, future in enumerate(as_completed(futures), 1):
                if not self.is_collecting:
                    # H·ªßy c√°c futures c√≤n l·∫°i
                    for f in futures:
                        f.cancel()
                    break
                
                try:
                    url = future.result(timeout=10)
                    if url:
                        if url not in self.collected_urls:
                            self.collected_urls.add(url)
                            new_urls_count += 1
                            logger.info(f"Th√™m URL m·ªõi: {url}")
                        else:
                            duplicate_count += 1
                    else:
                        error_count += 1
                except Exception as e:
                    error_count += 1
                    logger.error(f"L·ªói trong thread: {e}")
                
                # Callback ti·∫øn tr√¨nh
                if progress_callback and i % 5 == 0:  # C·∫≠p nh·∫≠t m·ªói 5 requests
                    progress_callback(i, num_requests, new_urls_count, duplicate_count, error_count)
        
        self.is_collecting = False
        return new_urls_count, duplicate_count, error_count
    
    def save_urls_to_file(self, filename: str = "urls.txt") -> Optional[str]:
        """L∆∞u URLs v√†o file - T·ªëi ∆∞u h√≥a"""
        try:
            with open(filename, 'w', encoding='utf-8', buffering=8192) as f:
                f.write(f"# URLs ƒë∆∞·ª£c thu th·∫≠p v√†o {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# T·ªïng c·ªông: {len(self.collected_urls)} URLs\n\n")
                
                # S·∫Øp x·∫øp v√† ghi m·ªôt l·∫ßn ƒë·ªÉ t·ªëi ∆∞u I/O
                sorted_urls = sorted(self.collected_urls)
                f.writelines(f"{url}\n" for url in sorted_urls)
            
            return filename
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u file: {e}")
            return None
    
    def clear_urls(self):
        """X√≥a t·∫•t c·∫£ URLs ƒë√£ thu th·∫≠p"""
        self.collected_urls.clear()
    
    def stop_collecting(self):
        """D·ª´ng qu√° tr√¨nh thu th·∫≠p"""
        self.is_collecting = False

# Kh·ªüi t·∫°o collector
collector = URLCollector()

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /start"""
    welcome_message = """
ü§ñ *Bot Thu Th·∫≠p URLs - Phi√™n B·∫£n T·ªëi ∆Øu*

C√°c l·ªánh c√≥ s·∫µn:
‚Ä¢ `/collect <api_url> <s·ªë_l·∫ßn> [url_key] [mode]` - Thu th·∫≠p URLs t·ª´ API
‚Ä¢ `/status` - Xem tr·∫°ng th√°i hi·ªán t·∫°i
‚Ä¢ `/download` - T·∫£i file urls.txt
‚Ä¢ `/clear` - X√≥a t·∫•t c·∫£ URLs ƒë√£ thu th·∫≠p
‚Ä¢ `/stop` - D·ª´ng qu√° tr√¨nh thu th·∫≠p
‚Ä¢ `/help` - Xem h∆∞·ªõng d·∫´n chi ti·∫øt

*Mode t·ªëi ∆∞u:*
‚Ä¢ `async` - S·ª≠ d·ª•ng async (nhanh nh·∫•t, m·∫∑c ƒë·ªãnh)
‚Ä¢ `thread` - S·ª≠ d·ª•ng threading (t∆∞∆°ng th√≠ch cao)

*V√≠ d·ª•:*
`/collect https://api.example.com/data 100 url async`
    """
    await update.message.reply_text(welcome_message, parse_mode='Markdown')

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /help"""
    help_text = """
üìñ *H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng - Phi√™n B·∫£n T·ªëi ∆Øu*

*L·ªánh collect:*
`/collect <api_url> <s·ªë_l·∫ßn> [url_key] [mode]`

‚Ä¢ `api_url`: URL c·ªßa API c·∫ßn g·ªçi
‚Ä¢ `s·ªë_l·∫ßn`: S·ªë l·∫ßn request API (1-2000)
‚Ä¢ `url_key`: Key ch·ª©a URL trong JSON (m·∫∑c ƒë·ªãnh: "url")
‚Ä¢ `mode`: Ch·∫ø ƒë·ªô x·ª≠ l√Ω (async/thread, m·∫∑c ƒë·ªãnh: async)

*T·ªëi ∆∞u h√≥a m·ªõi:*
‚Ä¢ ‚ö° Async batch processing - nhanh h∆°n 5-10 l·∫ßn
‚Ä¢ üîÑ Connection pooling - t√°i s·ª≠ d·ª•ng k·∫øt n·ªëi
‚Ä¢ üßµ Threading fallback - ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch
‚Ä¢ üìä Real-time progress tracking
‚Ä¢ üíæ Optimized file I/O

*V√≠ d·ª•:*
‚Ä¢ `/collect https://picsum.photos/200/300 100 url async`
‚Ä¢ `/collect https://api.example.com/data 200 download_url thread`
‚Ä¢ `/collect https://randomuser.me/api 50 picture`

*L∆∞u √Ω:*
‚Ä¢ Ch·∫ø ƒë·ªô async nhanh nh·∫•t cho API ·ªïn ƒë·ªãnh
‚Ä¢ Ch·∫ø ƒë·ªô thread t·ªët cho API c√≥ ƒë·ªô tr·ªÖ cao
‚Ä¢ T·ªëi ƒëa 2000 requests m·ªói l·∫ßn (tƒÉng t·ª´ 1000)
‚Ä¢ Auto retry v√† error handling
    """
    await update.message.reply_text(help_text, parse_mode='Markdown')

async def collect_urls_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /collect - T·ªëi ∆∞u h√≥a"""
    if len(context.args) < 2:
        await update.message.reply_text(
            "‚ùå S·ª≠ d·ª•ng: `/collect <api_url> <s·ªë_l·∫ßn> [url_key] [mode]`",
            parse_mode='Markdown'
        )
        return
    
    if collector.is_collecting:
        await update.message.reply_text("‚ö†Ô∏è Bot ƒëang thu th·∫≠p URLs. D√πng `/stop` ƒë·ªÉ d·ª´ng tr∆∞·ªõc.")
        return
    
    try:
        api_url = context.args[0]
        num_requests = int(context.args[1])
        url_key = context.args[2] if len(context.args) > 2 else "url"
        mode = context.args[3].lower() if len(context.args) > 3 else "async"
        
        if num_requests < 1 or num_requests > 2000:
            await update.message.reply_text("‚ùå S·ªë l·∫ßn request ph·∫£i t·ª´ 1-2000")
            return
            
        if mode not in ["async", "thread"]:
            mode = "async"
        
        # G·ª≠i th√¥ng b√°o b·∫Øt ƒë·∫ßu
        start_time = time.time()
        progress_msg = await update.message.reply_text(
            f"üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p URLs (Ch·∫ø ƒë·ªô: {mode.upper()})..."
        )
        
        # Bi·∫øn ƒë·ªÉ theo d√µi progress
        last_update_time = 0
        
        def update_progress(current, total, new_count, duplicate_count, error_count):
            nonlocal last_update_time
            current_time = time.time()
            
            # C·∫≠p nh·∫≠t UI m·ªói 2 gi√¢y ho·∫∑c khi ho√†n th√†nh
            if current_time - last_update_time >= 2 or current == total:
                last_update_time = current_time
                
                elapsed = current_time - start_time
                rate = current / elapsed if elapsed > 0 else 0
                eta = (total - current) / rate if rate > 0 else 0
                
                progress_text = f"üìä Ti·∫øn tr√¨nh: {current}/{total} ({current/total*100:.1f}%)\n"
                progress_text += f"‚ö° T·ªëc ƒë·ªô: {rate:.1f} req/s\n"
                progress_text += f"‚è±Ô∏è C√≤n l·∫°i: {eta:.0f}s\n"
                progress_text += f"‚úÖ URLs m·ªõi: {new_count}\n"
                progress_text += f"üîÑ Tr√πng l·∫∑p: {duplicate_count}\n"
                progress_text += f"‚ùå L·ªói: {error_count}"
                
                update_progress.last_text = progress_text
        
        # Thu th·∫≠p URLs v·ªõi ch·∫ø ƒë·ªô ƒë∆∞·ª£c ch·ªçn
        if mode == "async":
            batch_size = min(50, max(10, num_requests // 10))  # Dynamic batch size
            new_count, duplicate_count, error_count = await collector.collect_urls_async_batch(
                api_url, num_requests, url_key, batch_size, update_progress
            )
        else:  # thread mode
            max_workers = min(20, max(5, num_requests // 10))  # Dynamic worker count
            new_count, duplicate_count, error_count = await asyncio.get_event_loop().run_in_executor(
                None, collector.collect_urls_threaded, api_url, num_requests, url_key, max_workers, update_progress
            )
        
        end_time = time.time()
        total_time = end_time - start_time
        avg_rate = num_requests / total_time if total_time > 0 else 0
        
        # G·ª≠i k·∫øt qu·∫£ v√† file
        if collector.collected_urls:
            try:
                filename = collector.save_urls_to_file()
                if filename and os.path.exists(filename):
                    # Caption v·ªõi th·ªëng k√™ chi ti·∫øt
                    caption = f"‚úÖ *Thu th·∫≠p ho√†n th√†nh!*\n\n"
                    caption += f"üìä K·∫øt qu·∫£:\n"
                    caption += f"‚Ä¢ URLs m·ªõi: {new_count}\n"
                    caption += f"‚Ä¢ Tr√πng l·∫∑p: {duplicate_count}\n"
                    caption += f"‚Ä¢ L·ªói: {error_count}\n"
                    caption += f"‚Ä¢ T·ªïng URLs: {len(collector.collected_urls)}\n\n"
                    caption += f"‚ö° Hi·ªáu su·∫•t:\n"
                    caption += f"‚Ä¢ Th·ªùi gian: {total_time:.1f}s\n"
                    caption += f"‚Ä¢ T·ªëc ƒë·ªô TB: {avg_rate:.1f} req/s\n"
                    caption += f"‚Ä¢ Ch·∫ø ƒë·ªô: {mode.upper()}"
                    
                    # G·ª≠i file
                    with open(filename, 'rb') as f:
                        await update.message.reply_document(
                            document=f,
                            filename="urls.txt",
                            caption=caption,
                            parse_mode='Markdown'
                        )
                    
                    os.remove(filename)
                    await progress_msg.delete()
                    
                else:
                    # Fallback text result
                    result_text = f"‚úÖ *Ho√†n th√†nh!* (L·ªói t·∫°o file)\n\n"
                    result_text += f"üìä URLs m·ªõi: {new_count} | Tr√πng: {duplicate_count} | L·ªói: {error_count}\n"
                    result_text += f"‚ö° {avg_rate:.1f} req/s trong {total_time:.1f}s"
                    await progress_msg.edit_text(result_text, parse_mode='Markdown')
                    
            except Exception as file_error:
                logger.error(f"File error: {file_error}")
                result_text = f"‚úÖ *Thu th·∫≠p ho√†n th√†nh!*\n\n"
                result_text += f"üìä URLs: {new_count} m·ªõi, {duplicate_count} tr√πng, {error_count} l·ªói\n"
                result_text += f"‚ö° {avg_rate:.1f} req/s\n‚ùå L·ªói g·ª≠i file, d√πng `/download`"
                await progress_msg.edit_text(result_text, parse_mode='Markdown')
        else:
            result_text = f"‚ö†Ô∏è *Kh√¥ng c√≥ URLs n√†o!*\n\n"
            result_text += f"Ki·ªÉm tra API URL v√† key name\n"
            result_text += f"‚ö° ƒê√£ test {num_requests} requests trong {total_time:.1f}s"
            await progress_msg.edit_text(result_text, parse_mode='Markdown')
        
    except ValueError:
        await update.message.reply_text("‚ùå S·ªë l·∫ßn request ph·∫£i l√† s·ªë nguy√™n")
    except Exception as e:
        logger.error(f"Collect error: {e}")
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")

async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /status"""
    status_text = f"üìä *Tr·∫°ng Th√°i Bot*\n\n"
    status_text += f"‚Ä¢ T·ªïng URLs: {len(collector.collected_urls)}\n"
    status_text += f"‚Ä¢ ƒêang thu th·∫≠p: {'‚úÖ' if collector.is_collecting else '‚ùå'}\n"
    status_text += f"‚Ä¢ Session: {'üü¢ Active' if collector.session and not collector.session.closed else 'üî¥ Closed'}\n"
    
    if collector.collected_urls:
        recent_urls = list(collector.collected_urls)[-3:]
        status_text += f"\n*URLs g·∫ßn nh·∫•t:*\n"
        for i, url in enumerate(recent_urls, 1):
            display_url = url[:50] + "..." if len(url) > 50 else url
            status_text += f"{i}. `{display_url}`\n"
    
    await update.message.reply_text(status_text, parse_mode='Markdown')

async def download_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /download"""
    if not collector.collected_urls:
        await update.message.reply_text("‚ùå Ch∆∞a c√≥ URLs n√†o ƒë∆∞·ª£c thu th·∫≠p")
        return
    
    try:
        filename = collector.save_urls_to_file()
        if filename and os.path.exists(filename):
            with open(filename, 'rb') as f:
                await update.message.reply_document(
                    document=f,
                    filename=filename,
                    caption=f"üìÅ File ch·ª©a {len(collector.collected_urls)} URLs"
                )
            os.remove(filename)
        else:
            await update.message.reply_text("‚ùå L·ªói khi t·∫°o file")
    except Exception as e:
        await update.message.reply_text(f"‚ùå L·ªói: {str(e)}")

async def clear_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /clear"""
    count = len(collector.collected_urls)
    collector.clear_urls()
    await collector.close_session()  # ƒê√≥ng session khi clear
    await update.message.reply_text(f"üóëÔ∏è ƒê√£ x√≥a {count} URLs v√† reset session")

async def stop_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """L·ªánh /stop"""
    if collector.is_collecting:
        collector.stop_collecting()
        await update.message.reply_text("‚èπÔ∏è ƒê√£ d·ª´ng qu√° tr√¨nh thu th·∫≠p URLs")
    else:
        await update.message.reply_text("‚ùå Kh√¥ng c√≥ qu√° tr√¨nh thu th·∫≠p n√†o ƒëang ch·∫°y")

def main():
    """H√†m main"""
    if not BOT_TOKEN or BOT_TOKEN == "YOUR_BOT_TOKEN_HERE":
        print("‚ùå Vui l√≤ng thay th·∫ø BOT_TOKEN b·∫±ng token th·ª±c t·ª´ @BotFather")
        return
    
    # T·∫°o application
    application = Application.builder().token(BOT_TOKEN).build()
    
    # Th√™m handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("collect", collect_urls_command))
    application.add_handler(CommandHandler("status", status_command))
    application.add_handler(CommandHandler("download", download_command))
    application.add_handler(CommandHandler("clear", clear_command))
    application.add_handler(CommandHandler("stop", stop_command))
    
    # Ch·∫°y bot
    print("ü§ñ Bot t·ªëi ∆∞u ƒëang ch·∫°y...")
    application.run_polling()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\nüëã Bot ƒë√£ d·ª´ng")
    finally:
        # Cleanup
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if not loop.is_closed():
                loop.run_until_complete(collector.close_session())
        except:
            pass
